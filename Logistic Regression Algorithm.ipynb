{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights before:\n",
      "[[ 0.02307651]\n",
      " [-0.00133954]\n",
      " [-0.00140673]\n",
      " [ 0.01225808]\n",
      " [ 0.02024576]]\n",
      "Predicted labels:[[1. 1. 1. 1. 1.]] Actual labels:[[1 1 1 1 0]]\n",
      "cost 0.2961152100957178\n",
      "Predicted labels:[[1. 1. 1. 1. 1.]] Actual labels:[[1 1 1 1 0]]\n",
      "cost 0.24264183304164036\n",
      "Predicted labels:[[1. 1. 1. 1. 1.]] Actual labels:[[1 1 1 1 0]]\n",
      "cost 0.21643068885809705\n",
      "Predicted labels:[[1. 1. 1. 1. 1.]] Actual labels:[[1 1 1 1 0]]\n",
      "cost 0.201966053567722\n",
      "Predicted labels:[[1. 1. 1. 1. 1.]] Actual labels:[[1 1 1 1 0]]\n",
      "cost 0.192912164405237\n",
      "Predicted labels:[[1. 1. 1. 1. 1.]] Actual labels:[[1 1 1 1 0]]\n",
      "cost 0.18656200155603817\n",
      "Predicted labels:[[1. 1. 1. 1. 1.]] Actual labels:[[1 1 1 1 0]]\n",
      "cost 0.18166352981525233\n",
      "Predicted labels:[[1. 1. 1. 1. 1.]] Actual labels:[[1 1 1 1 0]]\n",
      "cost 0.17759405564098918\n",
      "Predicted labels:[[1. 1. 1. 1. 1.]] Actual labels:[[1 1 1 1 0]]\n",
      "cost 0.17402498257301874\n",
      "Predicted labels:[[1. 1. 1. 1. 1.]] Actual labels:[[1 1 1 1 0]]\n",
      "cost 0.17077454871872286\n",
      "Predicted labels:[[1. 1. 1. 1. 1.]] Actual labels:[[1 1 1 1 0]]\n",
      "cost 0.1677383743443194\n",
      "Predicted labels:[[1. 1. 1. 1. 1.]] Actual labels:[[1 1 1 1 0]]\n",
      "cost 0.16485467515265206\n",
      "Predicted labels:[[1. 1. 1. 1. 1.]] Actual labels:[[1 1 1 1 0]]\n",
      "cost 0.1620859466350192\n",
      "Predicted labels:[[1. 1. 1. 1. 1.]] Actual labels:[[1 1 1 1 0]]\n",
      "cost 0.15940891559555218\n",
      "Predicted labels:[[1. 1. 1. 1. 1.]] Actual labels:[[1 1 1 1 0]]\n",
      "cost 0.15680883771612916\n",
      "Predicted labels:[[1. 1. 1. 1. 1.]] Actual labels:[[1 1 1 1 0]]\n",
      "cost 0.15427617056066295\n",
      "Predicted labels:[[1. 1. 1. 1. 1.]] Actual labels:[[1 1 1 1 0]]\n",
      "cost 0.15180458809936864\n",
      "Predicted labels:[[1. 1. 1. 1. 1.]] Actual labels:[[1 1 1 1 0]]\n",
      "cost 0.14938977389729027\n",
      "Predicted labels:[[1. 1. 1. 1. 1.]] Actual labels:[[1 1 1 1 0]]\n",
      "cost 0.1470286767279027\n",
      "Predicted labels:[[1. 1. 1. 1. 1.]] Actual labels:[[1 1 1 1 0]]\n",
      "cost 0.14471904608117792\n",
      "Predicted labels:[[1. 1. 1. 1. 1.]] Actual labels:[[1 1 1 1 0]]\n",
      "cost 0.1424591397681004\n",
      "Predicted labels:[[1. 1. 1. 1. 1.]] Actual labels:[[1 1 1 1 0]]\n",
      "cost 0.1402475387106492\n",
      "Predicted labels:[[1. 1. 1. 1. 1.]] Actual labels:[[1 1 1 1 0]]\n",
      "cost 0.1380830291861944\n",
      "Predicted labels:[[1. 1. 1. 1. 1.]] Actual labels:[[1 1 1 1 0]]\n",
      "cost 0.13596452787217478\n",
      "Predicted labels:[[1. 1. 1. 1. 1.]] Actual labels:[[1 1 1 1 0]]\n",
      "cost 0.1338910342174937\n",
      "Predicted labels:[[1. 1. 1. 1. 1.]] Actual labels:[[1 1 1 1 0]]\n",
      "cost 0.13186160033773003\n",
      "Predicted labels:[[1. 1. 1. 1. 1.]] Actual labels:[[1 1 1 1 0]]\n",
      "cost 0.1298753121764546\n",
      "Predicted labels:[[1. 1. 1. 1. 1.]] Actual labels:[[1 1 1 1 0]]\n",
      "cost 0.12793127791388462\n",
      "Predicted labels:[[1. 1. 1. 1. 1.]] Actual labels:[[1 1 1 1 0]]\n",
      "cost 0.12602862103004217\n",
      "Predicted labels:[[1. 1. 1. 1. 1.]] Actual labels:[[1 1 1 1 0]]\n",
      "cost 0.12416647634404869\n",
      "Predicted labels:[[1. 1. 1. 1. 1.]] Actual labels:[[1 1 1 1 0]]\n",
      "cost 0.12234398794094388\n",
      "Predicted labels:[[1. 1. 1. 1. 1.]] Actual labels:[[1 1 1 1 0]]\n",
      "cost 0.12056030827948012\n",
      "Predicted labels:[[1. 1. 1. 1. 1.]] Actual labels:[[1 1 1 1 0]]\n",
      "cost 0.11881459802269818\n",
      "Predicted labels:[[1. 1. 1. 1. 1.]] Actual labels:[[1 1 1 1 0]]\n",
      "cost 0.1171060262949473\n",
      "Predicted labels:[[1. 1. 1. 1. 1.]] Actual labels:[[1 1 1 1 0]]\n",
      "cost 0.11543377117465545\n",
      "Predicted labels:[[1. 1. 1. 1. 1.]] Actual labels:[[1 1 1 1 0]]\n",
      "cost 0.11379702030116154\n",
      "Predicted labels:[[1. 1. 1. 1. 1.]] Actual labels:[[1 1 1 1 0]]\n",
      "cost 0.11219497151897777\n",
      "Predicted labels:[[1. 1. 1. 1. 1.]] Actual labels:[[1 1 1 1 0]]\n",
      "cost 0.11062683351222735\n",
      "Predicted labels:[[1. 1. 1. 1. 1.]] Actual labels:[[1 1 1 1 0]]\n",
      "cost 0.10909182640110134\n",
      "Predicted labels:[[1. 1. 1. 1. 1.]] Actual labels:[[1 1 1 1 0]]\n",
      "cost 0.10758918228453238\n",
      "Predicted labels:[[1. 1. 1. 1. 1.]] Actual labels:[[1 1 1 1 0]]\n",
      "cost 0.10611814572122036\n",
      "Predicted labels:[[1. 1. 1. 1. 1.]] Actual labels:[[1 1 1 1 0]]\n",
      "cost 0.10467797414617837\n",
      "Predicted labels:[[1. 1. 1. 1. 1.]] Actual labels:[[1 1 1 1 0]]\n",
      "cost 0.10326793822311102\n",
      "Predicted labels:[[1. 1. 1. 1. 1.]] Actual labels:[[1 1 1 1 0]]\n",
      "cost 0.10188732213484297\n",
      "Predicted labels:[[1. 1. 1. 1. 1.]] Actual labels:[[1 1 1 1 0]]\n",
      "cost 0.10053542381511622\n",
      "Predicted labels:[[1. 1. 1. 1. 0.]] Actual labels:[[1 1 1 1 0]]\n",
      "cost 0.09921155512566247\n",
      "Predicted labels:[[1. 1. 1. 1. 0.]] Actual labels:[[1 1 1 1 0]]\n",
      "cost 0.09791504198270774\n",
      "Predicted labels:[[1. 1. 1. 1. 0.]] Actual labels:[[1 1 1 1 0]]\n",
      "cost 0.09664522443711213\n",
      "Predicted labels:[[1. 1. 1. 1. 0.]] Actual labels:[[1 1 1 1 0]]\n",
      "cost 0.09540145671226474\n",
      "Predicted labels:[[1. 1. 1. 1. 0.]] Actual labels:[[1 1 1 1 0]]\n",
      "cost 0.09418310720368957\n",
      "Weight after logistic regression:\n",
      "[[ 2.34693308]\n",
      " [ 1.16571646]\n",
      " [-0.377153  ]\n",
      " [ 0.06057168]\n",
      " [-0.27909578]]\n",
      "Bias after logistic regression: 0.07639678184297233\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(20)\n",
    "\n",
    "# Defining inputs(X) with their labels(y)\n",
    "x = np.random.rand(5,5)\n",
    "y = np.array([1,1,1,1,0])\n",
    "y = y.reshape(1,5)\n",
    "m = y.size #Number of training examples\n",
    "\n",
    "\n",
    "# Initializing weights, bias and learning rate for logistic regression\n",
    "w = np.random.randn(5,1) * 0.01\n",
    "print('Weights before:')\n",
    "print(w)\n",
    "b = 0\n",
    "lr = 0.5\n",
    "iterations = 50\n",
    "\n",
    "for i in range(iterations):\n",
    "    # Function to predict (Not in binary form)\n",
    "    z = np.dot(w.T, x) + b \n",
    "\n",
    "    # Predicted y hat (applying the sigmoid function)\n",
    "    a = 1 / (1 + np.exp(-z))\n",
    "    print('Predicted labels:' + str(np.round(a)), 'Actual labels:' + str(y))\n",
    "    \n",
    "    # Calculating the loss\n",
    "    loss = -(y * np.log10(a) + (1-y) * np.log10(1-a))\n",
    "\n",
    "    # Calculating the cost function\n",
    "    cost = (np.sum(loss))/(m)\n",
    "    print('cost', cost)\n",
    "\n",
    "    # Gradient descent\n",
    "    dz = np.subtract(a,y)\n",
    "\n",
    "    db = np.sum(dz)/(m)\n",
    "\n",
    "    dw = (np.dot(x, dz.T))/m\n",
    "    \n",
    "    #Updating weights and bias\n",
    "    w = w - np.dot(lr, dw)\n",
    "    b = b - np.dot(lr, db)\n",
    "    \n",
    "print('Weight after logistic regression:')\n",
    "print(w)\n",
    "print('Bias after logistic regression:', b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
