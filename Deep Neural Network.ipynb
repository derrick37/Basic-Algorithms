{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels:[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]] Actual labels:[[1. 0. 0. 1. 0. 1. 1. 0. 0. 1.]]\n",
      "cost 0.32724479834358766\n",
      "cost 0.320316799051637\n",
      "cost 0.31477704303733695\n",
      "cost 0.3101540047012672\n",
      "cost 0.30616057087633264\n",
      "cost 0.3027782315329442\n",
      "cost 0.2997266650259415\n",
      "cost 0.2967586705151811\n",
      "cost 0.2937960634799285\n",
      "cost 0.2906057021486958\n",
      "cost 0.28726913256258957\n",
      "cost 0.28366265361209536\n",
      "cost 0.27921208859277663\n",
      "cost 0.2752226941911139\n",
      "cost 0.2700192036717509\n",
      "cost 0.2657457913598279\n",
      "cost 0.2590455605556107\n",
      "cost 0.2527096761832829\n",
      "cost 0.2485019554232896\n",
      "cost 0.2419467121574884\n",
      "cost 0.23520489924924096\n",
      "cost 0.22841093173159055\n",
      "cost 0.2334246721597854\n",
      "cost 0.20773040645713364\n",
      "cost 0.22068053710323193\n",
      "cost 0.2000854049789646\n",
      "cost 0.23053286005408946\n",
      "cost 0.1804844946256141\n",
      "cost 0.19937168597471516\n",
      "cost 0.23458337210421304\n",
      "cost 0.18900338690584556\n",
      "cost 0.15944229541289973\n",
      "cost 0.15198286355218427\n",
      "cost 0.14589823790595832\n",
      "cost 0.14453696557418516\n",
      "cost 0.16494843556601285\n",
      "cost 0.13703180058374045\n",
      "cost 0.12680785881081905\n",
      "cost 0.1247475069227987\n",
      "cost 0.11530499971709915\n",
      "cost 0.11276332836137799\n",
      "cost 0.1110087617088407\n",
      "cost 0.1047057958814335\n",
      "cost 0.0979908450932683\n",
      "cost 0.09549970706794479\n",
      "cost 0.10396502565589216\n",
      "cost 0.09148677886950918\n",
      "cost 0.0850658652199697\n",
      "cost 0.08104383849336808\n",
      "cost 0.0769946632939808\n",
      "Predicted labels:[[1. 0. 0. 1. 0. 1. 1. 0. 0. 1.]] Actual labels:[[1. 0. 0. 1. 0. 1. 1. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "# Defining inputs(X) with their labels(y)\n",
    "x = np.random.rand(100,10)\n",
    "y = np.round(np.random.rand(1,10))\n",
    "y = y.reshape(1,10)\n",
    "m = y.size #Number of training examples\n",
    "\n",
    "# Initializing weights, bias for the 4 layers\n",
    "w1 = np.random.randn(4,100) * 0.1\n",
    "w2 = np.random.randn(3,4) \n",
    "w3 = np.random.randn(2,3)\n",
    "w4 = np.random.randn(1,2) * 0.001\n",
    "b1 = np.zeros((4,1))\n",
    "b2 = np.zeros((3,1))\n",
    "b3 = np.array([[0.3],[0.5]])\n",
    "b4 = 0.7\n",
    "\n",
    "# Set learning rate and number of iterations\n",
    "lr = 0.15\n",
    "iterations = 50\n",
    "\n",
    "\n",
    "#Function to obtain derivative of the relu activation function\n",
    "def drelu(z):\n",
    "    return np.where(z<0, z*0, z*0+1)\n",
    "\n",
    "for i in range(iterations):\n",
    "    # Function to predict before applying activation function\n",
    "    z1 = np.dot(w1, x) + b1 \n",
    "    \n",
    "    # Predicted a1 (applying the relu activation function)\n",
    "    a1 = np.maximum(0, z1)\n",
    "    da1 = z1\n",
    "    da1 = drelu(da1)\n",
    "   \n",
    "    # 2nd layer with relu activation function\n",
    "    z2 = np.dot(w2, a1) + b2\n",
    "    a2 = np.maximum(0, z2)\n",
    "    da2 = z2\n",
    "    da2 = drelu(da2)\n",
    "        \n",
    "    # 3rd layer with relu activation function\n",
    "    z3 = np.dot(w3, a2) + b3\n",
    "    a3 = np.maximum(0, z3)\n",
    "    da3 = z3\n",
    "    da3 = drelu(da3)\n",
    "     \n",
    "    # 4th (output) layer with sigmoid activation function\n",
    "    z4 = np.dot(w4, a3) + b4\n",
    "    a4 = 1 / (1 + np.exp(-z4))\n",
    "    \n",
    "    if i==0:\n",
    "        print('Predicted labels:' + str(np.round(a4)), 'Actual labels:' + str(y))\n",
    "        \n",
    "    # Derivatve of the sigmoid activation function\n",
    "    da4 = (a4 * (1-a4))\n",
    "    \n",
    "    # Calculating the loss\n",
    "    loss = -1 * ((y * np.log10(a4)) + ((1-y) * np.log10((1-a4))))\n",
    "\n",
    "    # Calculating the cost function\n",
    "    cost = (np.sum(loss))/(m)\n",
    "    print('cost', cost)\n",
    "\n",
    "    # Gradient descent\n",
    "    dz4 = ((-y/a4) + ((1-y)/(1-a4))) * da4\n",
    "    dw4 = np.dot(dz4, a3.T)/(m)\n",
    "    db4 = (np.sum(dz4, axis=1, keepdims = True))/(m)\n",
    "    \n",
    "    dz3 = (np.dot(w4.T,dz4)) * (da3)\n",
    "    dw3 = np.dot(dz3, a2.T)/(m)\n",
    "    db3 = (np.sum(dz3, axis=1, keepdims = True))/(m)\n",
    "    \n",
    "    dz2 = (np.dot(w3.T,dz3)) * (da2)\n",
    "    dw2 = np.dot(dz2, a1.T)/(m)\n",
    "    db2 = (np.sum(dz2, axis=1, keepdims = True))/(m)\n",
    "    \n",
    "    dz1 = (np.dot(w2.T,dz2)) * (da1)\n",
    "    dw1 = np.dot(dz1, x.T)/(m)\n",
    "    db1 = (np.sum(dz1, axis=1, keepdims = True))/(m)\n",
    "\n",
    "    # Updating weights and bias\n",
    "    w1 = w1 - np.dot(lr, dw1)\n",
    "    b1 = b1 - np.dot(lr, db1)\n",
    "    w2 = w2 - np.dot(lr, dw2)\n",
    "    b2 = b2 - np.dot(lr, db2)\n",
    "    w3 = w3 - np.dot(lr, dw3)\n",
    "    b3 = b3 - np.dot(lr, db3)\n",
    "    w4 = w4 - np.dot(lr, dw4)\n",
    "    b4 = b4 - np.dot(lr, db4)\n",
    "    \n",
    "print('Predicted labels:' + str(np.round(a4)), 'Actual labels:' + str(y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
